name: Update RSS Feed
on:
  schedule:
    - cron: '0 */12 * * *'  # UTC 0/12点运行
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      # 1. 强制禁用缓存，深度克隆仓库
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # 拉取全部提交历史 <button class="citation-flag" data-index="6">
          clean: true     # 清除本地缓存文件 <button class="citation-flag" data-index="6">

      # 2. 设置Python环境
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: false   # 禁用Python依赖缓存 <button class="citation-flag" data-index="5">

      # 3. 安装依赖（禁用缓存）
      - run: pip install --no-cache-dir requests==2.31.0 beautifulsoup4==4.12.2 lxml==4.9.3  # <button class="citation-flag" data-index="5">

      # 4. 清理系统级缓存
      - run: |
          sudo apt-get clean
          sudo rm -rf /var/lib/apt/lists/*  # 清除Ubuntu包缓存 <button class="citation-flag" data-index="4">

      # 5. 处理黑名单文件
      - run: |
          python -c "
          import sys

          # 从 deleted_urls.txt 提取 URL
          deleted_urls = set()
          try:
              with open('deleted_urls.txt', 'r', encoding='utf-8') as f:
                  for line in f:
                      if '&&' in line:
                          _, url = line.strip().split('&&', 1)
                          deleted_urls.add(url.strip())
          except FileNotFoundError:
              pass

          # 保存纯URL到临时文件（避免格式污染）<button class="citation-flag" data-index="3">
          with open('deleted_urls.pure.txt', 'w', encoding='utf-8') as f:
              f.write('\n'.join(deleted_urls))"

      # 6. 核心抓取逻辑（强制清除HTTP缓存）
      - run: |
          python -c "
          import requests
          from bs4 import BeautifulSoup
          from urllib.parse import urlparse, parse_qs

          # 读取现有URL（防重复）<button class="citation-flag" data-index="5">
          existing = set()
          try:
              with open('rss.txt', 'r', encoding='utf-8') as f:
                  existing = {line.split('&&')[1].strip() for line in f if '&&' in line}
          except FileNotFoundError:
              pass

          # 读取纯URL黑名单 <button class="citation-flag" data-index="6">
          deleted = set()
          try:
              with open('deleted_urls.pure.txt', 'r', encoding='utf-8') as f:
                  deleted = {line.strip() for line in f}
          except FileNotFoundError:
              pass

          new_entries = []

          with open('rss_sources.txt', 'r') as f:
              rss_list = [url.strip() for url in f if url.strip()]

          for rss in rss_list:
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                      'Referer': 'https://www.ximalaya.com/',
                      'Cache-Control': 'no-cache, no-store, must-revalidate',  # 强制清除HTTP缓存 <button class="citation-flag" data-index="1"><button class="citation-flag" data-index="2">
                      'Pragma': 'no-cache',
                      'Expires': '0'
                  }
                  response = requests.get(rss, headers=headers, timeout=10)
                  response.raise_for_status()
                  soup = BeautifulSoup(response.content, 'xml')

                  for item in soup.find_all('item'):
                      title = item.title.text.strip().replace(' ', '_')
                      enclosure = item.find('enclosure')
                      if enclosure and enclosure.get('url', '').endswith('.m4a'):
                          raw_url = enclosure['url']
                          parsed = urlparse(raw_url)
                          jt_url = parse_qs(parsed.query).get('jt', [raw_url])[0].split('?')[0]
                          
                          # 严格过滤逻辑 <button class="citation-flag" data-index="6"><button class="citation-flag" data-index="9">
                          if jt_url not in existing and jt_url not in deleted:
                              new_entries.append(f\"{title}&&{jt_url}\")
                          else:
                              print(f'Filtered: {jt_url}')  # 调试日志 <button class="citation-flag" data-index="2">

              except Exception as e:
                  print(f'Error processing {rss}: {str(e)}')

          # 追加新条目到文件末尾 <button class="citation-flag" data-index="5">
          if new_entries:
              with open('rss.txt', 'a', encoding='utf-8') as f:
                  f.write('\n'.join(new_entries) + '\n')"

      # 7. 提交并强制同步远程分支
      - name: Commit changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git config pull.rebase false  # 避免冲突 <button class="citation-flag" data-index="4">
          git add rss.txt deleted_urls.pure.txt
          git commit -m "Update RSS $(date +'%Y-%m-%d %H:%M')" || echo "No changes"
          git pull origin main || true  # 先拉取远程更新 <button class="citation-flag" data-index="4">
          git push origin main --force  # 强制推送（慎用，仅用于修复冲突）<button class="citation-flag" data-index="6">
