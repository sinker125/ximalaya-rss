name: Update RSS Feed
on:
  schedule:
    - cron: '0 */12 * * *'  # UTC 0/12点运行
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout latest code
        uses: actions/checkout@v4
        with:
          ref: main
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade requests beautifulsoup4 lxml python-dateutil
          pip install requests==2.31.0 beautifulsoup4==4.12.2 lxml==4.9.3 python-dateutil==2.8.2 <button class="citation-flag" data-index="5"><button class="citation-flag" data-index="6">

      - name: Extract pure URLs from deleted_urls.txt
        run: |
          python -c "
          deleted_urls = set()
          try:
              with open('deleted_urls.txt', 'r', encoding='utf-8') as f:
                  for line in f:
                      if '&&' in line:
                          _, url = line.strip().split('&&', 1)
                          deleted_urls.add(url.strip())
          except FileNotFoundError:
              pass
          with open('deleted_urls.pure.txt', 'w', encoding='utf-8') as f:
              f.write('\n'.join(deleted_urls))"

      - name: Update RSS feed
        run: |
          python -c "
          import requests, time
          from bs4 import BeautifulSoup
          from urllib.parse import urlparse, parse_qs
          from dateutil.parser import parse  # 新增时间解析依赖 <button class="citation-flag" data-index="5">
          
          existing = set()
          try:
              with open('rss.txt', 'r', encoding='utf-8') as f:
                  existing = {line.split('&&')[1].strip() for line in f if '&&' in line}
          except FileNotFoundError:
              pass

          deleted = set()
          try:
              with open('deleted_urls.pure.txt', 'r', encoding='utf-8') as f:
                  deleted = {line.strip() for line in f}
          except FileNotFoundError:
              pass

          new_entries = []  # 存储元组 (pub_date, title, url)

          with open('rss_sources.txt', 'r') as f:
              rss_list = [url.strip() for url in f if url.strip()]

          for rss in rss_list:
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                  'Referer': 'https://www.ximalaya.com/',
                  'Cache-Control': 'no-cache, no-store, must-revalidate',
                  'Pragma': 'no-cache',
                  'Expires': '0'
              }
              # 添加时间戳参数强制刷新缓存 <button class="citation-flag" data-index="5">
              response = requests.get(f'{rss}?_t={int(time.time())}', headers=headers, timeout=10)
              response.raise_for_status()
              soup = BeautifulSoup(response.content, 'xml')

              for item in soup.find_all('item'):
                  try:
                      title = item.title.text.strip().replace(' ', '_')
                      enclosure = item.find('enclosure')
                      if enclosure and enclosure.get('url', '').endswith('.m4a'):
                          raw_url = enclosure['url']
                          parsed = urlparse(raw_url)
                          jt_url = parse_qs(parsed.query).get('jt', [raw_url])[0].split('?')[0]

                          # 提取发布时间并解析 <button class="citation-flag" data-index="5">
                          pub_date_str = item.pubDate.text.strip() if item.pubDate else ''
                          pub_date = parse(pub_date_str) if pub_date_str else None

                          if jt_url not in existing and jt_url not in deleted:
                              new_entries.append((pub_date, title, jt_url))
                          else:
                              print(f'Skipped: {jt_url}')
                  except Exception as e:
                      print(f'Error parsing item: {str(e)}')

          # 合并旧条目并排序
          try:
              with open('rss.txt', 'r', encoding='utf-8') as f:
                  existing_lines = [line.strip() for line in f]
          except FileNotFoundError:
              existing_lines = []

          all_entries = []
          # 新条目按时间排序
          new_sorted = sorted([e for e in new_entries if e[0]], key=lambda x: x[0], reverse=True)
          # 旧条目保留原格式
          old_lines = [line for line in existing_lines if line.split('&&')[1] not in deleted]

          # 合并并去重
          url_set = set()
          for entry in new_sorted:
              if entry[2] not in url_set and entry[2] not in deleted:
                  all_entries.append(f\"{entry[1]}&&{entry[2]}\")
                  url_set.add(entry[2])
          for line in old_lines:
              if line.split('&&')[1] not in url_set:
                  all_entries.append(line)

          # 写入文件时覆盖写入并按时间排序 <button class="citation-flag" data-index="5">
          with open('rss.txt', 'w', encoding='utf-8') as f:
              f.write('\n'.join(all_entries) + '\n')"

      - name: Commit changes
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add rss.txt deleted_urls.pure.txt
          git commit -m "Update RSS $(date +'%Y-%m-%d %H:%M')" || echo "No changes"
          git push origin main
